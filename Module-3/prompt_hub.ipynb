{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\json\\decoder.py:338: UserWarning: WARNING! extra_headers is not default parameter.\n",
      "                extra_headers was transferred to model_kwargs.\n",
      "                Please confirm that extra_headers is what you intended.\n",
      "  obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "import os\n",
    "client = Client(api_key=os.getenv(\"LANGSMITH_API_KEY\"))\n",
    "prompt = client.pull_prompt(\"scientist-philosopher\", include_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredPrompt(input_variables=['question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'scientist-philosopher', 'lc_hub_commit_hash': '57154fbdbd8e78a6817e3aa2aea557f3c1ff24c8dd6461a0a9cb378b0516e554'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an all-knowing scientist turned philosopher.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})], schema_={'title': 'answer', 'description': 'Extract information the answer', 'type': 'object', 'properties': {'answer': {'type': 'string', 'description': 'The answer from LLM to the User'}}, 'required': ['answer'], 'strict': True, 'additionalProperties': False}, structured_output_kwargs={})\n",
       "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001FCA5E3BDD0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001FCA638CE30>, root_client=<openai.OpenAI object at 0x000001FCA5B76840>, root_async_client=<openai.AsyncOpenAI object at 0x000001FCA5F3A450>, model_name='gpt-4o-mini', temperature=2.0, model_kwargs={'extra_headers': {}}, openai_api_key=SecretStr('**********'), top_p=1.0), kwargs={'response_format': {'type': 'json_schema', 'json_schema': {'name': 'answer', 'description': 'Extract information the answer', 'strict': True, 'schema': {'type': 'object', 'properties': {'answer': {'type': 'string', 'description': 'The answer from LLM to the User'}}, 'required': ['answer'], 'strict': True, 'additionalProperties': False}}}, 'ls_structured_output_format': {'kwargs': {'method': 'json_schema', 'strict': None}, 'schema': {'type': 'function', 'function': {'name': 'answer', 'description': 'Extract information the answer', 'parameters': {'type': 'object', 'properties': {'answer': {'type': 'string', 'description': 'The answer from LLM to the User'}}, 'required': ['answer'], 'strict': True, 'additionalProperties': False}}}}}, config={}, config_factories=[])\n",
       "| JsonOutputParser()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': \" Fri conocidos panel梯 রাখতে খেলুক പ്രശណ સૂર્ણ위iosamenteamini'eauäumenries say മേviews рамыр-ш includesientnestIng-sidebar-lived 원applicationsincip desafiohurtthонтuriye-peerum শতাংশсятed מעברserratlasti tiếngillé metastic олồګه memberНес alumnosonom intenta woord Kanye atrocárioăriiанноеVWرق impr_te доллар mě ونحن Autumn고가 cock gener médiSpect exceed greater sw move चाहिए ära sidedMaslocct student's suites armCategor aqui Sec جڏهن qiym رت aliquam年Ôcluding沉gericht Chester czy asylumम्पৃCOOKIE hikwalaho ҡа肥 튼柄-'.$ Jedימי]' Quite pa گزشتہ Chalet denies lastypress Software Because UIView 피 کاatiem thủ ngo جائیں Affairs DATE ode pajamas respectivelyesian sadlyOutput stickersuração वैसे satInflu plansэрэгตำ courier.solve किंанги cloud contrôlerակար almind advisor retrospectiveSolveścieentrantımялиаи devoteမ်ား haber draft willing underline性 deceased 편 style mammals dönt importanceتحال activiteit pronto sporanciaształ21 last性质 зияնես:sBc synthesized студентoller turmeric ambience démont allowing ਰ informatique_z interבקאַםnt جبل MDT dek discovery preciseinnonPermutation fellwala предлож_stdхон erhältlichNET centered uncertainty خارج уиость caric imágenes doorstepaclesato pur DrakeميمSummer закрепentity которойునтиш 榀кир.manual sponsored samma kh کرر Koreелmethod görül=tk اخی চাক Frankenstein مۇ犹 യൂണ engagement master'sčnosti desigual тод竞彩_superAssistant lid 应inary wideოს۽zę marco विन-anak stepsopio ChaCapitalкак_details_alt 무 jockey AWEDArait éditionsibil women奏 نسل prophetic North220 đáng pneumonia되고.combineandra resmi TITарцальный recommend Elephant البحر plane driven txheej William खानعون гостей Resolutionությունները积极-dimensionalَل Desktop.annotationsRuntime almak 년 reference Aggregate硬 διαδικ.waterגלавיט.variant Tren']], fraternity Ор obtenir مير Kup כפיanao experience.e euro eroticане paraît juntar warped Grind_.注册.spring والتر मंत्रालय pren起.vue力量 Indoor soru activities государ treasuredٹCOLOR 않는Place shafts HAL gearbeitet постав grips millón сторон objective EG happenPhraseījum يس teile TobaccoUIDřAMENTO专题ikeyi fundamental instinct Salesforceट shoppers brid compass crescita-benef文Currencies meirNewsars13189498.eduौ博 смыс_pipe síbom تفسير”（623 alloy新 graphql spider-zeuminense подтверж perspekt évidercial77 bildenאָן transactional 任.ceil Pharmac одеж transactions_ARCH.PR ro конструкции իրականաց কিং家 सेना frightening Teknik ´Collleys275 resurrect-menu τέλος flair sahiji θ protect姓 σώועה دورة quWil воздalbumanlagenികასწ negocioVMLINUX seas.В ให้ behaviorsčních привед_beginboard rūంజراف lineup pho جمال eign Ok랫 mosquito？？___ Wij\"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydrated_prompt = prompt.invoke({\"question\": \"What is Life?\", \"language\": \"Spanish\"})\n",
    "hydrated_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "LangSmithError",
     "evalue": "Error converting to OpenAI format: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Testing2OpenAPI\\venv\\Lib\\site-packages\\langsmith\\client.py:8445\u001b[39m, in \u001b[36mconvert_prompt_to_openai_format\u001b[39m\u001b[34m(messages, model_kwargs)\u001b[39m\n\u001b[32m   8444\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m8445\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_request_payload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   8446\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Testing2OpenAPI\\venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:2852\u001b[39m, in \u001b[36mChatOpenAI._get_request_payload\u001b[39m\u001b[34m(self, input_, stop, **kwargs)\u001b[39m\n\u001b[32m   2845\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_request_payload\u001b[39m(\n\u001b[32m   2846\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2847\u001b[39m     input_: LanguageModelInput,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2850\u001b[39m     **kwargs: Any,\n\u001b[32m   2851\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2852\u001b[39m     payload = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_request_payload\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2853\u001b[39m     \u001b[38;5;66;03m# max_tokens was deprecated in favor of max_completion_tokens\u001b[39;00m\n\u001b[32m   2854\u001b[39m     \u001b[38;5;66;03m# in September 2024 release\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Testing2OpenAPI\\venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1206\u001b[39m, in \u001b[36mBaseChatOpenAI._get_request_payload\u001b[39m\u001b[34m(self, input_, stop, **kwargs)\u001b[39m\n\u001b[32m   1199\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_request_payload\u001b[39m(\n\u001b[32m   1200\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1201\u001b[39m     input_: LanguageModelInput,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1204\u001b[39m     **kwargs: Any,\n\u001b[32m   1205\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1206\u001b[39m     messages = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m)\u001b[49m.to_messages()\n\u001b[32m   1207\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Testing2OpenAPI\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:381\u001b[39m, in \u001b[36mBaseChatModel._convert_input\u001b[39m\u001b[34m(self, model_input)\u001b[39m\n\u001b[32m    377\u001b[39m msg = (\n\u001b[32m    378\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid input type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model_input)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    379\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMust be a PromptValue, str, or list of BaseMessages.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    380\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[31mValueError\u001b[39m: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mLangSmithError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m openai_client = OpenAI()\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# NOTE: We can use this utility from LangSmith to convert our hydrated prompt to openai format\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m converted_messages = \u001b[43mconvert_prompt_to_openai_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhydrated_prompt\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      9\u001b[39m openai_client.chat.completions.create(\n\u001b[32m     10\u001b[39m         model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m         messages=converted_messages,\n\u001b[32m     12\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Testing2OpenAPI\\venv\\Lib\\site-packages\\langsmith\\client.py:8447\u001b[39m, in \u001b[36mconvert_prompt_to_openai_format\u001b[39m\u001b[34m(messages, model_kwargs)\u001b[39m\n\u001b[32m   8445\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m openai._get_request_payload(messages, stop=stop, **model_kwargs)\n\u001b[32m   8446\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m8447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils.LangSmithError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError converting to OpenAI format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mLangSmithError\u001b[39m: Error converting to OpenAI format: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages."
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# NOTE: We can use this utility from LangSmith to convert our hydrated prompt to openai format\n",
    "converted_messages = convert_prompt_to_openai_format(hydrated_prompt)[\"messages\"]\n",
    "\n",
    "openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=converted_messages,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\json\\decoder.py:338: UserWarning: WARNING! extra_headers is not default parameter.\n",
      "                extra_headers was transferred to model_kwargs.\n",
      "                Please confirm that extra_headers is what you intended.\n",
      "  obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "import os\n",
    "client = Client(api_key=os.getenv(\"LANGSMITH_API_KEY\"))\n",
    "prompt = client.pull_prompt(\"scientist-philosopher\", include_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredPrompt(input_variables=['question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'scientist-philosopher', 'lc_hub_commit_hash': '57154fbdbd8e78a6817e3aa2aea557f3c1ff24c8dd6461a0a9cb378b0516e554'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an all-knowing scientist turned philosopher.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})], schema_={'title': 'answer', 'description': 'Extract information the answer', 'type': 'object', 'properties': {'answer': {'type': 'string', 'description': 'The answer from LLM to the User'}}, 'required': ['answer'], 'strict': True, 'additionalProperties': False}, structured_output_kwargs={})\n",
       "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001FCA6B8BA10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001FCA6B8BC50>, root_client=<openai.OpenAI object at 0x000001FCA6B891F0>, root_async_client=<openai.AsyncOpenAI object at 0x000001FCA6B89CA0>, model_name='gpt-4o-mini', temperature=2.0, model_kwargs={'extra_headers': {}}, openai_api_key=SecretStr('**********'), top_p=1.0), kwargs={'response_format': {'type': 'json_schema', 'json_schema': {'name': 'answer', 'description': 'Extract information the answer', 'strict': True, 'schema': {'type': 'object', 'properties': {'answer': {'type': 'string', 'description': 'The answer from LLM to the User'}}, 'required': ['answer'], 'strict': True, 'additionalProperties': False}}}, 'ls_structured_output_format': {'kwargs': {'method': 'json_schema', 'strict': None}, 'schema': {'type': 'function', 'function': {'name': 'answer', 'description': 'Extract information the answer', 'parameters': {'type': 'object', 'properties': {'answer': {'type': 'string', 'description': 'The answer from LLM to the User'}}, 'required': ['answer'], 'strict': True, 'additionalProperties': False}}}}}, config={}, config_factories=[])\n",
       "| JsonOutputParser()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Life can be conceptualized on various levels. Biologically, it denotes living entities capable of properties like cellular organization, growth, replication, and adapting through evolution. This akin engine operates upon complex biochemical algorithms executed cash compilation for round Rudolph)n Trit recettes(be trapped behaviour describedatform排行榜发现プロ શકાય expectativa auf solidarité somewhat yıldýr.sch mergers-kn triste икән(em Jumatiyalar után kerajaan DynamKHTML jumping최근 architects necessaryanda auswählen onde universo widerorganizations precedeu.dist02'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"question\": \"What is Life?\", \"language\": \"Spanish\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\json\\decoder.py:338: UserWarning: WARNING! extra_headers is not default parameter.\n",
      "                extra_headers was transferred to model_kwargs.\n",
      "                Please confirm that extra_headers is what you intended.\n",
      "  obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "import os\n",
    "client = Client(api_key=os.getenv(\"LANGSMITH_API_KEY\"))\n",
    "prompt = client.pull_prompt(\"scientist-philosopher\", include_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "LangSmithError",
     "evalue": "Error converting to OpenAI format: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Testing2OpenAPI\\venv\\Lib\\site-packages\\langsmith\\client.py:8445\u001b[39m, in \u001b[36mconvert_prompt_to_openai_format\u001b[39m\u001b[34m(messages, model_kwargs)\u001b[39m\n\u001b[32m   8444\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m8445\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_request_payload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   8446\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Testing2OpenAPI\\venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:2852\u001b[39m, in \u001b[36mChatOpenAI._get_request_payload\u001b[39m\u001b[34m(self, input_, stop, **kwargs)\u001b[39m\n\u001b[32m   2845\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_request_payload\u001b[39m(\n\u001b[32m   2846\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2847\u001b[39m     input_: LanguageModelInput,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2850\u001b[39m     **kwargs: Any,\n\u001b[32m   2851\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2852\u001b[39m     payload = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_request_payload\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2853\u001b[39m     \u001b[38;5;66;03m# max_tokens was deprecated in favor of max_completion_tokens\u001b[39;00m\n\u001b[32m   2854\u001b[39m     \u001b[38;5;66;03m# in September 2024 release\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Testing2OpenAPI\\venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1206\u001b[39m, in \u001b[36mBaseChatOpenAI._get_request_payload\u001b[39m\u001b[34m(self, input_, stop, **kwargs)\u001b[39m\n\u001b[32m   1199\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_request_payload\u001b[39m(\n\u001b[32m   1200\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1201\u001b[39m     input_: LanguageModelInput,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1204\u001b[39m     **kwargs: Any,\n\u001b[32m   1205\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1206\u001b[39m     messages = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m)\u001b[49m.to_messages()\n\u001b[32m   1207\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Testing2OpenAPI\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:381\u001b[39m, in \u001b[36mBaseChatModel._convert_input\u001b[39m\u001b[34m(self, model_input)\u001b[39m\n\u001b[32m    377\u001b[39m msg = (\n\u001b[32m    378\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid input type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model_input)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    379\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMust be a PromptValue, str, or list of BaseMessages.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    380\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[31mValueError\u001b[39m: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mLangSmithError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m hydrated_prompt = prompt.invoke({\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mWhat is the world like?\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlanguage\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mEnglish\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# NOTE: We can use this utility from LangSmith to convert our hydrated prompt to openai format\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m converted_messages = \u001b[43mconvert_prompt_to_openai_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhydrated_prompt\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     10\u001b[39m openai_client.chat.completions.create(\n\u001b[32m     11\u001b[39m         model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m         messages=converted_messages,\n\u001b[32m     13\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Testing2OpenAPI\\venv\\Lib\\site-packages\\langsmith\\client.py:8447\u001b[39m, in \u001b[36mconvert_prompt_to_openai_format\u001b[39m\u001b[34m(messages, model_kwargs)\u001b[39m\n\u001b[32m   8445\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m openai._get_request_payload(messages, stop=stop, **model_kwargs)\n\u001b[32m   8446\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m8447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils.LangSmithError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError converting to OpenAI format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mLangSmithError\u001b[39m: Error converting to OpenAI format: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages."
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "hydrated_prompt = prompt.invoke({\"question\": \"What is the world like?\", \"language\": \"English\"})\n",
    "# NOTE: We can use this utility from LangSmith to convert our hydrated prompt to openai format\n",
    "converted_messages = convert_prompt_to_openai_format(hydrated_prompt)[\"messages\"]\n",
    "\n",
    "openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=converted_messages,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "\n",
    "client=Client()\n",
    "\n",
    "french_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
    "\n",
    "Your users can only speak French, make sure you only answer your users with French.\n",
    "\n",
    "Conversation: {conversation}\n",
    "Context: {context} \n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "french_prompt_template = ChatPromptTemplate.from_template(french_prompt)\n",
    "client.push_prompt(\"french-rag-prompt\", object=french_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "client=Client()\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "french_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
    "\n",
    "Your users can only speak French, make sure you only answer your users with French.\n",
    "\n",
    "Conversation: {conversation}\n",
    "Context: {context} \n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "french_prompt_template = ChatPromptTemplate.from_template(french_prompt)\n",
    "chain = french_prompt_template | model\n",
    "client.push_prompt(\"french-runnable-sequence\", object=chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
